{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZOKKfRtdD00"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# --- PHASE I: DATA PREPARATION & CAMPAIGN-READY STAGING ---\n",
        "# ==========================================\n",
        "# Ingesting raw JSON telemetry and staging enriched dataframes for behavioral analysis\n",
        "\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Configure standard logging for pipeline health\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def ingest_telemetry(file_pattern: str, data_dir: str = '.') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ingests Cowrie JSON telemetry files into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_pattern: The glob pattern for the JSON log files.\n",
        "        data_dir: The relative directory containing the telemetry files.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The raw telemetry dataframe with parsed timestamps.\n",
        "    \"\"\"\n",
        "    paths = sorted(Path(data_dir).glob(file_pattern))\n",
        "    data_list = []\n",
        "    error_count = 0\n",
        "\n",
        "    for file_path in paths:\n",
        "        with file_path.open('r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data_list.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    error_count += 1\n",
        "                    continue\n",
        "\n",
        "    if error_count > 0:\n",
        "        logger.warning(\"Skipped %d malformed JSON lines during ingestion.\", error_count)\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    if not df.empty:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "def stage_command_telemetry(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Isolates shell interactions and stages unique command playbooks.\n",
        "\n",
        "    Args:\n",
        "        df: The base telemetry dataframe.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing the command dataframe and the session sequences dataframe.\n",
        "    \"\"\"\n",
        "    cmd_df = df[df['eventid'] == 'cowrie.command.input'].copy()\n",
        "    cmd_df = cmd_df.sort_values(['session', 'timestamp'])\n",
        "\n",
        "    # Calculate temporal deltas for ML features\n",
        "    cmd_df['time_delta'] = cmd_df.groupby('session')['timestamp'].diff().dt.total_seconds()\n",
        "\n",
        "    # Sequence Fingerprinting for Phase XIV Campaign Correlation\n",
        "    cmd_df['input_clean'] = cmd_df['input'].astype(str).str.strip()\n",
        "    session_sequences = cmd_df.groupby('session')['input_clean'].apply(lambda x: \" | \".join(x)).reset_index()\n",
        "    session_sequences.columns = ['session', 'command_sequence']\n",
        "\n",
        "    return cmd_df, session_sequences\n",
        "\n",
        "def harvest_credentials(df: pd.DataFrame, top_n: int = 25) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates brute-force attempts to identify top credential pairs.\n",
        "\n",
        "    Args:\n",
        "        df: The base telemetry dataframe.\n",
        "        top_n: Number of top credentials to return.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe of top aggregated credentials.\n",
        "    \"\"\"\n",
        "    creds = df[df['eventid'].isin(['cowrie.login.success', 'cowrie.login.failed'])].copy()\n",
        "    top_creds = creds.groupby(['username', 'password']).size().reset_index(name='count')\n",
        "    return top_creds.sort_values('count', ascending=False).head(top_n)\n",
        "\n",
        "def print_mission_summary(df: pd.DataFrame, cmd_df: pd.DataFrame, top_creds: pd.DataFrame, session_sequences: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted operational summary of the staged telemetry.\"\"\"\n",
        "    start_time = df['timestamp'].min()\n",
        "    end_time = df['timestamp'].max()\n",
        "    soak_duration = (end_time - start_time).total_seconds() / 3600\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### TELEMETRY STAGING COMPLETE: MISSION DATA READY ###\")\n",
        "    print(f\"Total Events Ingested: {len(df):,}\")\n",
        "    print(f\"Operational Soak Time: {soak_duration:.1f} Hours\")\n",
        "    print(f\"Event Density: {len(df)/soak_duration:.1f} events/hour\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"  * Command Telemetry: {len(cmd_df):,} records staged.\")\n",
        "    print(f\"  * Credential Dictionary: {len(top_creds):,} unique pairs identified.\")\n",
        "    print(f\"  * Campaign Sequences: {len(session_sequences):,} playbooks mapped.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- EXECUTION BLOCK ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Pointing to a standard data repository structure\n",
        "    DATA_DIRECTORY = '.'\n",
        "    FILE_PATTERN = 'cowrie.json.2026-02-0*'\n",
        "\n",
        "    # 1 & 2. Telemetry Ingestion & Master DF Construction\n",
        "    df = ingest_telemetry(file_pattern=FILE_PATTERN, data_dir=DATA_DIRECTORY)\n",
        "\n",
        "    if not df.empty:\n",
        "        # 3 & 5. Command Staging & Sequence Fingerprinting\n",
        "        cmd_df, session_sequences = stage_command_telemetry(df)\n",
        "\n",
        "        # 4. Credential Harvesting\n",
        "        top_creds = harvest_credentials(df)\n",
        "\n",
        "        # 6. Operational Summary\n",
        "        print_mission_summary(df, cmd_df, top_creds, session_sequences)\n",
        "    else:\n",
        "        logger.error(\"CRITICAL ERROR: NO TELEMETRY FOUND.\")\n",
        "        logger.info(\"Action Required: Verify FILE_PATTERN and ensure cowrie.json files exist in the data directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# --- PHASE II: ENRICHMENT (Geospatial Attribution) ---\n",
        "# ==========================================\n",
        "# Enriching raw IP telemetry with geographic metadata for regional risk profiling\n",
        "\n",
        "import time\n",
        "import logging\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "\n",
        "# Reuse the logger from Phase I if integrated, otherwise configure a new one\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "IP_API_URL = \"http://ip-api.com/json/{ip}\"\n",
        "REQUEST_TIMEOUT = 5\n",
        "API_SLEEP_DELAY = 0.5\n",
        "TOP_N_IPS = 20\n",
        "\n",
        "def fetch_country_metadata(ip: str, session: requests.Session) -> str:\n",
        "    \"\"\"\n",
        "    Fetches country metadata for a given IP with defensive error handling.\n",
        "\n",
        "    Args:\n",
        "        ip: The target IP address.\n",
        "        session: An active requests.Session object for connection pooling.\n",
        "\n",
        "    Returns:\n",
        "        str: The resolved country name, or an error/status string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = session.get(IP_API_URL.format(ip=ip), timeout=REQUEST_TIMEOUT)\n",
        "\n",
        "        # Handle API Throttling (HTTP 429)\n",
        "        if response.status_code == 429:\n",
        "            logger.warning(\"Rate limit hit for IP: %s\", ip)\n",
        "            return 'Rate Limited'\n",
        "\n",
        "        response.raise_for_status()  # Catch other HTTP errors (404, 500, etc.)\n",
        "        data = response.json()\n",
        "        return data.get('country', 'Unknown')\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(\"Network error fetching geo-data for IP %s: %s\", ip, e)\n",
        "        return 'Lookup Failed'\n",
        "    except ValueError:\n",
        "        logger.error(\"Failed to parse JSON response for IP %s\", ip)\n",
        "        return 'Lookup Failed'\n",
        "\n",
        "def enrich_geospatial_data(df: pd.DataFrame, top_n: int = TOP_N_IPS) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enriches the dataframe with geographic metadata for top volume sources.\n",
        "\n",
        "    Args:\n",
        "        df: The telemetry dataframe containing a 'src_ip' column.\n",
        "        top_n: Number of high-volume IPs to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The enriched dataframe.\n",
        "    \"\"\"\n",
        "    if 'src_ip' not in df.columns:\n",
        "        logger.error(\"CRITICAL: 'src_ip' column missing from dataframe. Skipping enrichment.\")\n",
        "        return df\n",
        "\n",
        "    unique_ips = df['src_ip'].value_counts().head(top_n).index.tolist()\n",
        "    logger.info(\"[*] Initializing Geospatial Enrichment for %d unique high-volume nodes...\", len(unique_ips))\n",
        "\n",
        "    geo_map: Dict[str, str] = {}\n",
        "\n",
        "    # Utilizing a Session context manager for efficient connection pooling\n",
        "    with requests.Session() as session:\n",
        "        for ip in unique_ips:\n",
        "            geo_map[ip] = fetch_country_metadata(ip, session)\n",
        "            # Sleep to respect API fair-use policies and ensure data integrity\n",
        "            time.sleep(API_SLEEP_DELAY)\n",
        "\n",
        "    # Map Intelligence back to Master Telemetry\n",
        "    df['country'] = df['src_ip'].map(geo_map).fillna('Other/Low Volume')\n",
        "    return df\n",
        "\n",
        "def print_geospatial_summary(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted operational summary of the geospatial attribution.\"\"\"\n",
        "    if 'country' not in df.columns:\n",
        "        return\n",
        "\n",
        "    enriched_mask = df['country'] != 'Other/Low Volume'\n",
        "    if not enriched_mask.any():\n",
        "        logger.warning(\"No geospatial data was successfully mapped.\")\n",
        "        return\n",
        "\n",
        "    top_origin = df.loc[enriched_mask, 'country'].value_counts().idxmax()\n",
        "    coverage_pct = (enriched_mask.sum() / len(df)) * 100\n",
        "    unique_regions = df.loc[enriched_mask, 'country'].nunique()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### GEOSPATIAL ENRICHMENT: ATTRIBUTION READY ###\")\n",
        "    print(f\"Enrichment Coverage: {coverage_pct:.1f}% of total event volume.\")\n",
        "    print(f\"Primary Regional Vector: {top_origin}\")\n",
        "    print(f\"Operational Observation: High-volume sources successfully mapped to {unique_regions} distinct regions.\")\n",
        "    print(\"Assessment: Attribution data successfully injected into master telemetry for Phase V mapping.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- EXECUTION BLOCK ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'df' exists from Phase I\n",
        "    try:\n",
        "        df = enrich_geospatial_data(df)\n",
        "        print_geospatial_summary(df)\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'df' is not defined. Ensure Phase I executed successfully.\")"
      ],
      "metadata": {
        "id": "BFFS_8fse8-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE III: ML PIPELINE (ANOMALY, CLUSTERING, EXPLAINABILITY)\n",
        "# ==========================================\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Initialize logger for Phase III\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ------------------------------------------\n",
        "# PHASE III-A: FEATURE ENGINEERING & ISOLATION FOREST\n",
        "# ------------------------------------------\n",
        "\n",
        "def extract_session_features(cmd_df: pd.DataFrame, min_deltas: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"Extracts behavioral features from command telemetry for ML modeling.\"\"\"\n",
        "    if cmd_df.empty or 'session' not in cmd_df.columns:\n",
        "        logger.warning(\"Command telemetry is empty or missing 'session'. Cannot extract features.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    session_features: List[Dict[str, Any]] = []\n",
        "\n",
        "    for session_id, group in cmd_df.groupby(\"session\"):\n",
        "        deltas = group[\"time_delta\"].dropna()\n",
        "\n",
        "        if len(deltas) < min_deltas:\n",
        "            continue\n",
        "\n",
        "        session_features.append({\n",
        "            \"session_id\": session_id,\n",
        "            \"mean_delta\": deltas.mean(),\n",
        "            \"entropy_delta\": entropy(np.histogram(deltas, bins=10)[0] + 1),\n",
        "            \"burst_ratio\": (deltas < 0.5).mean(),\n",
        "            \"command_complexity\": group['input'].astype(str).str.len().mean(),\n",
        "            \"unique_commands\": group['input'].nunique()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(session_features)\n",
        "\n",
        "def detect_anomalies(features_df: pd.DataFrame, contamination_rate: float = 0.10, min_samples: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Applies an Isolation Forest to detect anomalous interactive sessions.\"\"\"\n",
        "    if len(features_df) < min_samples:\n",
        "        logger.warning(\"PIPELINE DEFERRED: Insufficient data volume for Isolation Forest (requires >= %d).\", min_samples)\n",
        "        features_df_fallback = features_df.copy()\n",
        "        features_df_fallback['anomaly_score'] = 1\n",
        "        features_df_fallback['predicted_label'] = \"Scanner/Low Volume\"\n",
        "        return features_df_fallback\n",
        "\n",
        "    feature_cols = [col for col in features_df.columns if col != \"session_id\"]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(features_df[feature_cols])\n",
        "\n",
        "    iso_forest = IsolationForest(contamination=contamination_rate, random_state=42)\n",
        "\n",
        "    results_df = features_df.copy()\n",
        "    results_df['anomaly_score'] = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def print_isolation_summary(features_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted operational summary of the Sentinel Triage phase.\"\"\"\n",
        "    if 'anomaly_score' not in features_df.columns:\n",
        "        return\n",
        "\n",
        "    anomalies_detected = len(features_df[features_df['anomaly_score'] == -1])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### PHASE III-A: SENTINEL TRIAGE COMPLETE ###\")\n",
        "    print(f\"Total Sessions Processed: {len(features_df)}\")\n",
        "    print(f\"High-Risk Outliers Isolated: {anomalies_detected}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# PHASE III-B: K-MEANS CLUSTERING & TAXONOMY\n",
        "# ------------------------------------------\n",
        "\n",
        "def evaluate_centroid_heuristics(centroids_df: pd.DataFrame) -> Dict[int, str]:\n",
        "    \"\"\"Evaluates cluster centroids to assign human-readable taxonomy labels.\"\"\"\n",
        "    cluster_labels = {}\n",
        "    for i, row in centroids_df.iterrows():\n",
        "        if row.get('burst_ratio', 0) > 0.5 and row.get('command_complexity', 0) > 0.5:\n",
        "            cluster_labels[i] = \"AI/LLM Agent (Burst-Think)\"\n",
        "        elif row.get('entropy_delta', 0) > 0.5:\n",
        "            cluster_labels[i] = \"Human\"\n",
        "        else:\n",
        "            cluster_labels[i] = \"Automated Bot\"\n",
        "    return cluster_labels\n",
        "\n",
        "def apply_kmeans_clustering(features_df: pd.DataFrame, n_clusters: int = 3, min_samples: int = 5) -> Tuple[pd.DataFrame, Dict[int, str]]:\n",
        "    \"\"\"Applies K-Means clustering to categorize session behaviors.\"\"\"\n",
        "    cluster_labels: Dict[int, str] = {}\n",
        "    results_df = features_df.copy()\n",
        "\n",
        "    if len(results_df) < min_samples:\n",
        "        logger.warning(\"PIPELINE DEFERRED: Insufficient data volume for K-Means (requires >= %d).\", min_samples)\n",
        "        if 'predicted_label' not in results_df.columns:\n",
        "            results_df['predicted_label'] = \"Scanner/Low Volume\"\n",
        "        return results_df, cluster_labels\n",
        "\n",
        "    exclude_cols = ['session_id', 'anomaly_score', 'predicted_label', 'cluster_id']\n",
        "    feature_cols = [col for col in results_df.columns if col not in exclude_cols]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(results_df[feature_cols])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    results_df['cluster_id'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    centroids_df = pd.DataFrame(kmeans.cluster_centers_, columns=feature_cols)\n",
        "    cluster_labels = evaluate_centroid_heuristics(centroids_df)\n",
        "    results_df['predicted_label'] = results_df['cluster_id'].map(cluster_labels)\n",
        "\n",
        "    return results_df, cluster_labels\n",
        "\n",
        "def print_taxonomy_summary(cluster_labels: Dict[int, str]) -> None:\n",
        "    \"\"\"Prints a formatted operational summary of the taxonomy mappings.\"\"\"\n",
        "    if not cluster_labels:\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### PHASE III-B: TAXONOMY CLUSTERING COMPLETE ###\")\n",
        "    print(\"Centroid Mappings Derived:\")\n",
        "    for cluster_id, label in cluster_labels.items():\n",
        "        print(f\"  -> Cluster {cluster_id} mapped to: {label}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# PHASE III-C: EXPLAINABILITY & DECISION TREE\n",
        "# ------------------------------------------\n",
        "\n",
        "def train_surrogate_explainer(features_df: pd.DataFrame, max_depth: int = 3, min_samples: int = 5) -> Tuple[Optional[DecisionTreeClassifier], Optional[str], Optional[str]]:\n",
        "    \"\"\"Trains a Decision Tree surrogate to explain the behavioral clustering labels.\"\"\"\n",
        "    if len(features_df) < min_samples:\n",
        "        logger.warning(\"PIPELINE DEFERRED: Insufficient data volume for Decision Tree Explainer (requires >= %d).\", min_samples)\n",
        "        return None, None, None\n",
        "\n",
        "    if 'predicted_label' not in features_df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: 'predicted_label' missing. Ensure Phase III-B ran successfully.\")\n",
        "        return None, None, None\n",
        "\n",
        "    if features_df['predicted_label'].nunique() <= 1:\n",
        "        logger.warning(\"Only one unique taxonomy label found. Surrogate tree cannot perform mathematical splits.\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "JQdwR7EYfQ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE IV: THREAT COMMAND SUMMARY GENERATION\n",
        "# ==========================================\n",
        "# Forensic audit of un-truncated command strings to determine tactical objectives\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Initialize logger for Phase IV\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- TACTICAL HEURISTICS ---\n",
        "RECON_KEYWORDS = ['uname', 'ls', 'whoami', 'id', 'cat /proc', 'ifconfig', 'netstat']\n",
        "PAYLOAD_KEYWORDS = ['wget', 'curl', 'tftp', 'ftpget', 'scp']\n",
        "\n",
        "def extract_top_commands(cmd_df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates and sorts un-truncated shell commands by frequency.\n",
        "\n",
        "    Args:\n",
        "        cmd_df: Dataframe containing command telemetry ('input' column).\n",
        "        top_n: Number of top commands to extract.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe of top commands and their frequencies.\n",
        "    \"\"\"\n",
        "    if cmd_df.empty or 'input' not in cmd_df.columns:\n",
        "        logger.warning(\"Command dataframe is empty or missing 'input'. Cannot extract dossier.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    top_cmds = cmd_df['input'].value_counts().head(top_n).reset_index()\n",
        "    top_cmds.columns = ['Full_Command', 'Frequency']\n",
        "    return top_cmds\n",
        "\n",
        "def evaluate_tactical_intent(top_cmds: pd.DataFrame, recon_keys: List[str] = RECON_KEYWORDS, payload_keys: List[str] = PAYLOAD_KEYWORDS) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Analyzes the primary command payloads to determine the tactical objective.\n",
        "\n",
        "    Args:\n",
        "        top_cmds: Dataframe of aggregated top commands.\n",
        "        recon_keys: List of keywords indicating reconnaissance.\n",
        "        payload_keys: List of keywords indicating payload delivery.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing the tactical assessment strings.\n",
        "    \"\"\"\n",
        "    if top_cmds.empty:\n",
        "        return {}\n",
        "\n",
        "    # Safely extract the most frequent command\n",
        "    primary_payload = str(top_cmds.iloc[0]['Full_Command']).lower()\n",
        "    top_frequency = top_cmds.iloc[0]['Frequency']\n",
        "\n",
        "    is_recon = any(k in primary_payload for k in recon_keys)\n",
        "    is_delivery = any(k in primary_payload for k in payload_keys)\n",
        "\n",
        "    # Determine Intelligence Summaries\n",
        "    if is_recon:\n",
        "        primary_vector = \"System Discovery/Reconnaissance\"\n",
        "        mitre_stage = \"Discovery\"\n",
        "    elif is_delivery:\n",
        "        primary_vector = \"Malware Ingress/Staging\"\n",
        "        mitre_stage = \"Execution\"\n",
        "    else:\n",
        "        primary_vector = \"General Probing\"\n",
        "        mitre_stage = \"Unknown/Initial Access\"\n",
        "\n",
        "    delivery_method = \"scripted/automated\" if top_frequency > 10 else \"bespoke/interactive\"\n",
        "\n",
        "    return {\n",
        "        \"primary_vector\": primary_vector,\n",
        "        \"delivery_method\": delivery_method,\n",
        "        \"mitre_stage\": mitre_stage\n",
        "    }\n",
        "\n",
        "def print_threat_summary(top_cmds: pd.DataFrame, total_unique_cmds: int, assessment: Dict[str, str]) -> None:\n",
        "    \"\"\"Prints a formatted operational dossier and tactical intelligence summary.\"\"\"\n",
        "    if top_cmds.empty or not assessment:\n",
        "        return\n",
        "\n",
        "    # 1. Forensic Header & Iterative Audit\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### FULL COMMAND DOSSIER: UN-TRUNCATED PAYLOAD ANALYSIS ###\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, row in top_cmds.iterrows():\n",
        "        print(f\"RANK: {i+1} | HITS: {row['Frequency']}\")\n",
        "        print(f\"PAYLOAD: {row['Full_Command']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # 2. Professional Intelligence Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### TACTICAL ASSESSMENT: ATTACKER INTENT AUDIT ###\")\n",
        "    print(f\"Total Command Diversity: {total_unique_cmds:,} unique strings harvested.\")\n",
        "    print(f\"Primary Vector: {assessment.get('primary_vector')}\")\n",
        "    print(f\"Operational Observation: The high frequency of the top payload indicates a {assessment.get('delivery_method')} delivery method.\")\n",
        "    print(f\"Assessment: Tactics are consistent with the '{assessment.get('mitre_stage')}' stage of the MITRE ATT&CK Matrix.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'cmd_df' exists from Phase I/II\n",
        "    try:\n",
        "        # 1. Extraction\n",
        "        top_cmds_df = extract_top_commands(cmd_df)\n",
        "\n",
        "        if not top_cmds_df.empty:\n",
        "            # 2. Assessment\n",
        "            total_unique = cmd_df['input'].nunique()\n",
        "            tactical_assessment = evaluate_tactical_intent(top_cmds_df)\n",
        "\n",
        "            # 3. Operational Summary\n",
        "            print_threat_summary(top_cmds_df, total_unique, tactical_assessment)\n",
        "        else:\n",
        "            logger.error(\"No valid commands found to generate Threat Summary.\")\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'cmd_df' is not defined. Ensure Phase I/II executed successfully.\")"
      ],
      "metadata": {
        "id": "lHh0hyY6gPdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE V: EXECUTIVE VISUALIZATION & REPORTING ENGINE\n",
        "# ==========================================\n",
        "# Generates high-impact visual analytics for behavioral classification,\n",
        "# tactical intent, geographic origins, and credential targeting.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "# Initialize logger for Phase V\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "TAXONOMY_ORDER = [\"Automated Bot\", \"Human\", \"AI/LLM Agent (Burst-Think)\"]\n",
        "PALETTE_STYLE = \"magma\"\n",
        "RECON_INDICATORS = ['uname', 'ls', 'whoami', 'id', 'cat /proc', 'netstat']\n",
        "MAX_DISPLAY_LEN = 55\n",
        "TOP_N_COUNTRIES = 10\n",
        "PALETTE_GEO = 'viridis'\n",
        "HEATMAP_CMAP = \"YlGnBu\"\n",
        "REQUIRED_COLS = ['username', 'password', 'count']\n",
        "\n",
        "# ==========================================\n",
        "# PHASE V-A: ATTACKER CLASSIFICATION (LOG SCALE)\n",
        "# ==========================================\n",
        "\n",
        "def prepare_census_data(cmd_df: pd.DataFrame, features_df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Merges command telemetry with behavioral labels to create a deduplicated census dataset.\"\"\"\n",
        "    if features_df.empty or 'predicted_label' not in features_df.columns:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: Pipeline requires valid 'predicted_label' column.\")\n",
        "        return None\n",
        "\n",
        "    if cmd_df.empty or 'session' not in cmd_df.columns:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: Command telemetry is empty or missing 'session'.\")\n",
        "        return None\n",
        "\n",
        "    viz_df = cmd_df.merge(\n",
        "        features_df[['session_id', 'predicted_label']],\n",
        "        left_on='session',\n",
        "        right_on='session_id',\n",
        "        how='inner'\n",
        "    )\n",
        "    return viz_df.drop_duplicates('session')\n",
        "\n",
        "def plot_attacker_census(viz_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a log-scale countplot of the behavioral taxonomy.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    ax = sns.countplot(\n",
        "        data=viz_df,\n",
        "        x='predicted_label',\n",
        "        hue='predicted_label',\n",
        "        palette=PALETTE_STYLE,\n",
        "        order=TAXONOMY_ORDER,\n",
        "        legend=False\n",
        "    )\n",
        "\n",
        "    total_sessions = len(viz_df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        if height > 0:\n",
        "            percentage = f'{(100 * height / total_sessions):.1f}%'\n",
        "            ax.annotate(\n",
        "                percentage,\n",
        "                (p.get_x() + p.get_width() / 2., height),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points',\n",
        "                fontsize=10, fontweight='bold'\n",
        "            )\n",
        "\n",
        "    ax.set_yscale(\"log\")\n",
        "    plt.title('Phase V-A: Attacker Classification Census (Logarithmic Distribution)', fontsize=14)\n",
        "    plt.xlabel('Behavioral Taxonomy')\n",
        "    plt.ylabel('Session Count (Log 10 Scale)')\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "    plt.ylim(top=ax.get_ylim()[1] * 2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_census_summary(viz_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the threat landscape.\"\"\"\n",
        "    total_classified = len(viz_df)\n",
        "    counts = viz_df['predicted_label'].value_counts()\n",
        "\n",
        "    bot_count = counts.get(\"Automated Bot\", 0)\n",
        "    human_count = counts.get(\"Human\", 0)\n",
        "    ai_count = counts.get(\"AI/LLM Agent (Burst-Think)\", 0)\n",
        "\n",
        "    noise_ratio = (bot_count / total_classified) * 100 if total_classified > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### BEHAVIORAL CENSUS: THREAT LANDSCAPE SUMMARY ###\")\n",
        "    print(f\"Total Classified Sessions: {total_classified:,}\")\n",
        "    print(f\"Background Radiation (Bots): {noise_ratio:.1f}%\")\n",
        "    print(f\"Verified Human Activity: {human_count:,} sessions\")\n",
        "    print(f\"AI/LLM Agent Activity: {ai_count:,} sessions\")\n",
        "\n",
        "    print(\"\\nOperational Assessment:\")\n",
        "    if ai_count == 0:\n",
        "        print(\"-> Threat Ceiling: No high-entropy 'Burst-Think' patterns detected.\")\n",
        "        print(\"-> Risk Profile: Environment is currently limited to scripted automated probes.\")\n",
        "    else:\n",
        "        print(f\"-> Strategic Finding: Advanced agentic behavior identified in {ai_count} sessions.\")\n",
        "\n",
        "    print(f\"Strategic Conclusion: Pipeline successfully filtered {noise_ratio:.1f}% of traffic as non-interactive noise.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# PHASE V-B: TOP 10 COMMANDS (CONTEXTUAL AWARENESS)\n",
        "# ==========================================\n",
        "\n",
        "def prepare_top_commands(cmd_df: pd.DataFrame, top_n: int = 10) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Cleans and aggregates shell interactions to identify the top executed commands.\"\"\"\n",
        "    if cmd_df.empty or 'input' not in cmd_df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Mission data missing or invalid 'input' column.\")\n",
        "        return None\n",
        "\n",
        "    valid_cmds = cmd_df['input'].dropna().astype(str).str.strip()\n",
        "    valid_cmds = valid_cmds[valid_cmds != \"\"]\n",
        "\n",
        "    if valid_cmds.empty:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: Telemetry contains only empty inputs.\")\n",
        "        return None\n",
        "\n",
        "    top_cmds = valid_cmds.value_counts().head(top_n).reset_index()\n",
        "    top_cmds.columns = ['Full_Command', 'Frequency']\n",
        "    top_cmds['Display_Label'] = top_cmds['Full_Command'].apply(\n",
        "        lambda x: f\"{x[:MAX_DISPLAY_LEN]}...\" if len(x) > MAX_DISPLAY_LEN else x\n",
        "    )\n",
        "\n",
        "    return top_cmds\n",
        "\n",
        "def plot_top_commands(top_cmds: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a horizontal bar plot of the top executed commands.\"\"\"\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    sns.barplot(\n",
        "        data=top_cmds,\n",
        "        x='Frequency',\n",
        "        y='Display_Label',\n",
        "        hue='Display_Label',\n",
        "        palette='flare',\n",
        "        legend=False,\n",
        "        edgecolor='black',\n",
        "        alpha=0.9\n",
        "    )\n",
        "\n",
        "    plt.title('Phase V-B: Tactical Intent Audit (Top 10 Harvested Commands)', fontsize=14)\n",
        "    plt.xlabel('Execution Count')\n",
        "    plt.ylabel('Command Snippet')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_command_cadence(top_cmds: pd.DataFrame) -> Tuple[str, bool, bool]:\n",
        "    \"\"\"Evaluates the tactical intent and cadence based on the top command distributions.\"\"\"\n",
        "    full_command_list = top_cmds['Full_Command'].tolist()\n",
        "    frequency_list = top_cmds['Frequency'].tolist()\n",
        "\n",
        "    primary_cmd = full_command_list[0]\n",
        "    has_recon = any(ind in str(primary_cmd).lower() for ind in RECON_INDICATORS)\n",
        "\n",
        "    if len(frequency_list) > 1:\n",
        "        is_standardized = frequency_list[0] > (frequency_list[1] * 2)\n",
        "    else:\n",
        "        is_standardized = True\n",
        "\n",
        "    return primary_cmd, has_recon, is_standardized\n",
        "\n",
        "def print_tactical_intelligence(primary_cmd: str, has_recon: bool, is_standardized: bool) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the command payload audit.\"\"\"\n",
        "    display_cmd = f\"{primary_cmd[:60]}...\" if len(primary_cmd) > 60 else primary_cmd\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### TACTICAL INTELLIGENCE: COMMAND PAYLOAD AUDIT ###\")\n",
        "    print(f\"Primary Vector: {display_cmd}\")\n",
        "    print(f\"Operational Observation: Command cadence focuses on {'Environment Discovery' if has_recon else 'Payload Delivery/Persistence'}.\")\n",
        "    print(f\"Assessment: The high frequency of the top command suggests a {'standardized automated probe' if is_standardized else 'diverse interactive session'}.\")\n",
        "    print(\"Conclusion: Tactics align with TTPs commonly associated with opportunistic cloud-native botnets.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# PHASE V-C: GEOGRAPHIC ORIGINS (GLOBAL THREAT MAP)\n",
        "# ==========================================\n",
        "\n",
        "def prepare_geo_data(df: pd.DataFrame, top_n: int = TOP_N_COUNTRIES) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Aggregates telemetry to find the top geographic origins of threat actors.\"\"\"\n",
        "    if df.empty or 'country' not in df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Master dataframe missing or lacks 'country' column.\")\n",
        "        return None\n",
        "\n",
        "    top_countries = df['country'].value_counts().head(top_n).reset_index()\n",
        "    top_countries.columns = ['Country', 'Event_Count']\n",
        "\n",
        "    if top_countries.empty:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: No geographic data available.\")\n",
        "        return None\n",
        "\n",
        "    return top_countries\n",
        "\n",
        "def plot_geo_threats(top_countries: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a horizontal bar plot of the top geopolitical risk vectors.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sns.barplot(\n",
        "        data=top_countries,\n",
        "        x='Event_Count',\n",
        "        y='Country',\n",
        "        hue='Country',\n",
        "        palette=PALETTE_GEO,\n",
        "        legend=False,\n",
        "        edgecolor='black',\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    plt.title('Phase V-C: Geospatial Threat Attribution (Top 10 Origins)', fontsize=14)\n",
        "    plt.xlabel('Aggregated Event Volume')\n",
        "    plt.ylabel('Attacker Country of Origin')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_geo_intel(top_countries: pd.DataFrame, total_events: int, total_unique_countries: int) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates the geographic concentration to generate operational intelligence.\"\"\"\n",
        "    primary_origin = top_countries.iloc[0]['Country']\n",
        "    primary_volume = top_countries.iloc[0]['Event_Count']\n",
        "    is_concentrated = primary_volume > (total_events / 2)\n",
        "\n",
        "    return {\n",
        "        \"primary_origin\": primary_origin,\n",
        "        \"primary_volume\": primary_volume,\n",
        "        \"total_countries\": total_unique_countries,\n",
        "        \"is_concentrated\": is_concentrated\n",
        "    }\n",
        "\n",
        "def print_geo_summary(intel: Dict[str, Any]) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the geospatial data.\"\"\"\n",
        "    if not intel:\n",
        "        return\n",
        "\n",
        "    concentration_status = 'highly concentrated' if intel.get('is_concentrated') else 'globally distributed'\n",
        "    primary_origin = intel.get('primary_origin')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### GEOSPATIAL INTELLIGENCE: REGIONAL RISK VECTORS ###\")\n",
        "    print(f\"Primary Origin Node: {primary_origin} ({intel.get('primary_volume'):,} events)\")\n",
        "    print(f\"Global Reach: {intel.get('total_countries')} distinct nations identified in harvest.\")\n",
        "    print(f\"Operational Observation: Traffic is {concentration_status}.\")\n",
        "    print(f\"Assessment: Geopolitical heatmap suggests a dominance of {primary_origin}-based cloud infrastructure in the current botnet lifecycle.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# PHASE V-D: TARGETED CREDENTIAL HEATMAP\n",
        "# ==========================================\n",
        "\n",
        "def prepare_credential_pivot(top_creds: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Pivots the aggregated credential pairs into a matrix for heatmap visualization.\"\"\"\n",
        "    if top_creds.empty:\n",
        "        logger.info(\"CREDENTIAL INTELLIGENCE: No login attempts captured during this window.\")\n",
        "        return None\n",
        "\n",
        "    missing_cols = [col for col in REQUIRED_COLS if col not in top_creds.columns]\n",
        "    if missing_cols:\n",
        "        logger.error(f\"CRITICAL ERROR: top_creds is missing required columns: {missing_cols}\")\n",
        "        return None\n",
        "\n",
        "    pivot_creds = top_creds.pivot(index='username', columns='password', values='count').fillna(0)\n",
        "    return pivot_creds\n",
        "\n",
        "def plot_credential_heatmap(pivot_creds: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a heatmap visualization of targeted brute-force pairs.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sns.heatmap(\n",
        "        pivot_creds,\n",
        "        annot=True,\n",
        "        fmt=\".0f\",\n",
        "        cmap=HEATMAP_CMAP,\n",
        "        cbar_kws={'label': 'Attempt Frequency'}\n",
        "    )\n",
        "\n",
        "    plt.title('Phase V-D: Credential Harvesting Heatmap (High-Frequency Targets)', fontsize=14)\n",
        "    plt.xlabel('Target Password')\n",
        "    plt.ylabel('Target Username')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_brute_force_intel(top_creds: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluates the credential dataset to determine brute-force entropy and strategy.\"\"\"\n",
        "    top_pair = top_creds.sort_values(by='count', ascending=False).iloc[0]\n",
        "    total_unique_creds = len(top_creds)\n",
        "    strategy = 'standard dictionary' if total_unique_creds > 5 else 'highly targeted'\n",
        "\n",
        "    return {\n",
        "        \"top_user\": top_pair['username'],\n",
        "        \"top_pass\": top_pair['password'],\n",
        "        \"total_unique\": total_unique_creds,\n",
        "        \"strategy\": strategy\n",
        "    }\n",
        "\n",
        "def print_credential_summary(intel: Dict[str, Any]) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the brute-force attacks.\"\"\"\n",
        "    if not intel:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"### CREDENTIAL INTELLIGENCE: NO DATA RECORDED ###\")\n",
        "        print(\"Observation: No login attempts were captured during this specific telemetry window.\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        return\n",
        "\n",
        "    top_user = intel.get('top_user')\n",
        "    top_pass = intel.get('top_pass')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### CREDENTIAL INTELLIGENCE: BRUTE-FORCE ENTROPY ###\")\n",
        "    print(f\"Primary Target Pair: {top_user} / {top_pass}\")\n",
        "    print(f\"Credential Diversity: {intel.get('total_unique')} distinct pairs in top-tier attempts.\")\n",
        "    print(f\"Operational Observation: Attackers are prioritizing '{top_user}' as the high-probability entry point.\")\n",
        "    print(f\"Assessment: Pattern indicates a {intel.get('strategy')} brute-force strategy.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION BLOCK (PHASES V-A to V-D)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'df', 'cmd_df', 'features_df', and 'top_creds' exist from Phases I-IV\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase V: Visual Analytics & Intelligence Generation...\")\n",
        "\n",
        "        # --- V-A: Attacker Classification ---\n",
        "        census_df = prepare_census_data(cmd_df, features_df)\n",
        "        if census_df is not None and not census_df.empty:\n",
        "            plot_attacker_census(census_df)\n",
        "            print_census_summary(census_df)\n",
        "\n",
        "        # --- V-B: Top 10 Commands ---\n",
        "        top_cmds_df = prepare_top_commands(cmd_df)\n",
        "        if top_cmds_df is not None:\n",
        "            plot_top_commands(top_cmds_df)\n",
        "            primary, recon_flag, std_flag = evaluate_command_cadence(top_cmds_df)\n",
        "            print_tactical_intelligence(primary, recon_flag, std_flag)\n",
        "\n",
        "        # --- V-C: Geographic Threat Map ---\n",
        "        top_countries_df = prepare_geo_data(df)\n",
        "        if top_countries_df is not None:\n",
        "            plot_geo_threats(top_countries_df)\n",
        "            total_events = len(df)\n",
        "            total_unique = df['country'].nunique()\n",
        "            geo_intel = evaluate_geo_intel(top_countries_df, total_events, total_unique)\n",
        "            print_geo_summary(geo_intel)\n",
        "\n",
        "        # --- V-D: Targeted Credential Heatmap ---\n",
        "\n",
        "        pivot_df = prepare_credential_pivot(top_creds)\n",
        "        if pivot_df is not None:\n",
        "            plot_credential_heatmap(pivot_df)\n",
        "            cred_intel = evaluate_brute_force_intel(top_creds)\n",
        "            print_credential_summary(cred_intel)\n",
        "        else:\n",
        "            print_credential_summary({})\n",
        "\n",
        "    except NameError as e:\n",
        "        logger.error(f\"CRITICAL ERROR: Missing dataframe dependencies from earlier phases. {e}\")"
      ],
      "metadata": {
        "id": "Aef4DDdagrWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE VI: TEMPORAL CADENCE & BURST ANALYSIS\n",
        "# ==========================================\n",
        "# Evaluating execution velocity to differentiate machine-speed automation from human latency\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Initialize logger for Phase VI\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "MAX_DELTA_SECONDS = 300\n",
        "MACHINE_SPEED_THRESHOLD = 1.0\n",
        "BOT_DOMINANCE_THRESHOLD = 80.0\n",
        "\n",
        "def prepare_cadence_data(cmd_df: pd.DataFrame, max_seconds: int = MAX_DELTA_SECONDS) -> Optional[pd.Series]:\n",
        "    \"\"\"\n",
        "    Validates and extracts valid time deltas for temporal execution analysis.\n",
        "\n",
        "    Args:\n",
        "        cmd_df: Dataframe containing command telemetry and 'time_delta'.\n",
        "        max_seconds: Threshold to filter out extreme outliers (e.g., long session pauses).\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A cleaned series of time deltas, or None if invalid.\n",
        "    \"\"\"\n",
        "    if cmd_df.empty or 'time_delta' not in cmd_df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Mission data missing or lacks 'time_delta' column.\")\n",
        "        return None\n",
        "\n",
        "    # Drop NaN (the first command of any session has no prior delta)\n",
        "    cadence_data = cmd_df['time_delta'].dropna()\n",
        "\n",
        "    # Filter extreme outliers for a clean, focused visualization\n",
        "    valid_cadence = cadence_data[cadence_data < max_seconds]\n",
        "\n",
        "    if valid_cadence.empty:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: Insufficient consecutive commands to map temporal velocity.\")\n",
        "        return None\n",
        "\n",
        "    return valid_cadence\n",
        "\n",
        "def plot_temporal_cadence(valid_cadence: pd.Series, threshold: float = MACHINE_SPEED_THRESHOLD) -> None:\n",
        "    \"\"\"\n",
        "    Generates a log-scaled histogram of command execution cadences.\n",
        "\n",
        "    Args:\n",
        "        valid_cadence: Series of valid time deltas between commands.\n",
        "        threshold: The visual line separating machine vs. human speed.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Using a log-scaled X-axis because bot speeds and human speeds span magnitudes\n",
        "    sns.histplot(\n",
        "        valid_cadence,\n",
        "        bins=40,\n",
        "        kde=True,\n",
        "        color='darkred',\n",
        "        log_scale=True,\n",
        "        edgecolor='black',\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    # Adding a visual threshold line for the \"Machine Speed Barrier\"\n",
        "    plt.axvline(\n",
        "        x=threshold,\n",
        "        color='blue',\n",
        "        linestyle='--',\n",
        "        linewidth=2.5,\n",
        "        label=f'Human/Machine Threshold ({threshold}s)'\n",
        "    )\n",
        "\n",
        "    plt.title('Phase VI: Temporal Cadence Audit (Execution Velocity)', fontsize=14)\n",
        "    plt.xlabel('Seconds Between Commands (Log 10 Scale)')\n",
        "    plt.ylabel('Command Frequency (Density)')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_cadence_intel(cadence_data: pd.Series, threshold: float = MACHINE_SPEED_THRESHOLD) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculates execution velocity metrics to profile the session behaviors.\n",
        "\n",
        "    Args:\n",
        "        cadence_data: Cleaned series of time deltas.\n",
        "        threshold: The temporal cutoff for machine-speed execution.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing cadence intelligence metrics.\n",
        "    \"\"\"\n",
        "    total_deltas = len(cadence_data)\n",
        "    machine_speed_hits = len(cadence_data[cadence_data < threshold])\n",
        "    human_speed_hits = total_deltas - machine_speed_hits\n",
        "\n",
        "    machine_ratio = (machine_speed_hits / total_deltas) * 100 if total_deltas > 0 else 0\n",
        "    avg_cadence = cadence_data.mean()\n",
        "\n",
        "    return {\n",
        "        \"total_deltas\": total_deltas,\n",
        "        \"machine_hits\": machine_speed_hits,\n",
        "        \"human_hits\": human_speed_hits,\n",
        "        \"machine_ratio\": machine_ratio,\n",
        "        \"human_ratio\": 100 - machine_ratio,\n",
        "        \"avg_cadence\": avg_cadence\n",
        "    }\n",
        "\n",
        "def print_cadence_summary(intel: Dict[str, Any], dominance_threshold: float = BOT_DOMINANCE_THRESHOLD) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the temporal cadence.\"\"\"\n",
        "    if not intel:\n",
        "        return\n",
        "\n",
        "    machine_ratio = intel.get('machine_ratio', 0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### BEHAVIORAL INTELLIGENCE: TEMPORAL CADENCE AUDIT ###\")\n",
        "    print(f\"Total Measured Intervals: {intel.get('total_deltas'):,} command transitions.\")\n",
        "    print(f\"Machine-Speed Execution (<1s): {machine_ratio:.1f}% ({intel.get('machine_hits'):,} commands)\")\n",
        "    print(f\"Human-Speed Execution (>1s): {intel.get('human_ratio', 0):.1f}% ({intel.get('human_hits'):,} commands)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"Operational Observation: The average execution delay across the harvest is {intel.get('avg_cadence', 0):.2f} seconds.\")\n",
        "\n",
        "    if machine_ratio > dominance_threshold:\n",
        "        print(f\"Assessment: The heavy concentration of sub-second execution ({machine_ratio:.1f}%) confirms an environment dominated by highly-scripted, non-interactive botnets.\")\n",
        "    else:\n",
        "        print(\"Assessment: The elevated presence of human-speed execution suggests interactive, 'hands-on-keyboard' reconnaissance is actively occurring.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'cmd_df' exists from Phase I\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase VI: Temporal Cadence & Burst Analysis...\")\n",
        "\n",
        "        # 1. Prepare Data\n",
        "        cadence_series = prepare_cadence_data(cmd_df)\n",
        "\n",
        "        if cadence_series is not None:\n",
        "            # 2. Visualize\n",
        "            plot_temporal_cadence(cadence_series)\n",
        "\n",
        "            # 3. Assess & Summarize\n",
        "            cadence_intel = evaluate_cadence_intel(cadence_series)\n",
        "            print_cadence_summary(cadence_intel)\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'cmd_df' is not defined. Ensure Phase I executed successfully.\")"
      ],
      "metadata": {
        "id": "uWH-4OCehfMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE VII: HASSH FINGERPRINTING (Technical Attribution)\n",
        "# ==========================================\n",
        "# Identifying the underlying software libraries and tooling used by threat actors\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Initialize logger for Phase VII\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "TOP_N_HASSH = 10\n",
        "PALETTE_HASSH = 'coolwarm'\n",
        "\n",
        "def extract_hassh_telemetry(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Isolates SSH client key exchange telemetry for fingerprinting.\n",
        "\n",
        "    Args:\n",
        "        df: Master telemetry dataframe.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing 'session', 'hassh', and 'src_ip', or None if invalid.\n",
        "    \"\"\"\n",
        "    if df.empty or 'eventid' not in df.columns or 'hassh' not in df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Master dataframe missing or lacks 'eventid'/'hassh' columns.\")\n",
        "        return None\n",
        "\n",
        "    # Filter for Key Exchange events and drop empty HASSH values\n",
        "    hassh_data = df[df['eventid'] == 'cowrie.client.kex'][['session', 'hassh', 'src_ip']].copy()\n",
        "    hassh_data = hassh_data.dropna(subset=['hassh'])\n",
        "\n",
        "    if hassh_data.empty:\n",
        "        logger.warning(\"ATTRIBUTION DEFERRED: No HASSH key exchange telemetry found in this harvest.\")\n",
        "        return None\n",
        "\n",
        "    return hassh_data\n",
        "\n",
        "def prepare_top_fingerprints(hassh_data: pd.DataFrame, top_n: int = TOP_N_HASSH) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates the most frequently observed SSH fingerprints.\n",
        "\n",
        "    Args:\n",
        "        hassh_data: Dataframe of extracted HASSH telemetry.\n",
        "        top_n: Number of top profiles to extract.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Aggregated top fingerprints and their frequencies.\n",
        "    \"\"\"\n",
        "    top_hassh = hassh_data['hassh'].value_counts().head(top_n).reset_index()\n",
        "    top_hassh.columns = ['HASSH_Fingerprint', 'Frequency']\n",
        "    return top_hassh\n",
        "\n",
        "def plot_hassh_fingerprints(top_hassh: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Generates a horizontal bar plot mapping the top software fingerprints.\n",
        "\n",
        "    Args:\n",
        "        top_hassh: Dataframe containing aggregated HASSH profiles.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Assigning y to hue and setting legend=False for Seaborn v0.14+ compliance\n",
        "    sns.barplot(\n",
        "        data=top_hassh,\n",
        "        x='Frequency',\n",
        "        y='HASSH_Fingerprint',\n",
        "        hue='HASSH_Fingerprint',\n",
        "        palette=PALETTE_HASSH,\n",
        "        legend=False,\n",
        "        edgecolor='black',\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    plt.title('Phase VII: SSH Client Fingerprints (Technical Attribution)', fontsize=14)\n",
        "    plt.xlabel('Session Count')\n",
        "    plt.ylabel('HASSH Fingerprint (MD5)')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_hassh_intel(hassh_data: pd.DataFrame, top_hassh: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the tooling profiles to determine attacker dependency on specific libraries.\n",
        "\n",
        "    Args:\n",
        "        hassh_data: The full extracted HASSH telemetry.\n",
        "        top_hassh: The aggregated top profiles.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing the derived technical attribution metrics.\n",
        "    \"\"\"\n",
        "    if top_hassh.empty:\n",
        "        return {}\n",
        "\n",
        "    primary_fingerprint = top_hassh.iloc[0]['HASSH_Fingerprint']\n",
        "    primary_frequency = top_hassh.iloc[0]['Frequency']\n",
        "\n",
        "    fingerprint_diversity = hassh_data['hassh'].nunique()\n",
        "    total_kex_events = len(hassh_data)\n",
        "\n",
        "    # Assess if the environment is dominated by a single automation tool\n",
        "    is_highly_dependent = primary_frequency > (total_kex_events / 2)\n",
        "\n",
        "    return {\n",
        "        \"primary_fingerprint\": primary_fingerprint,\n",
        "        \"diversity\": fingerprint_diversity,\n",
        "        \"dependency_level\": 'high' if is_highly_dependent else 'moderate'\n",
        "    }\n",
        "\n",
        "def print_hassh_summary(intel: Dict[str, Any]) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the technical attribution.\"\"\"\n",
        "    if not intel:\n",
        "        return\n",
        "\n",
        "    primary_fp = intel.get('primary_fingerprint', 'N/A')\n",
        "    short_fp = f\"{primary_fp[:8]}...\" if len(primary_fp) > 8 else primary_fp\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### TECHNICAL ATTRIBUTION: TOOLING PROFILE AUDIT ###\")\n",
        "    print(f\"Primary Client Profile: {primary_fp}\")\n",
        "    print(f\"Fingerprint Diversity: {intel.get('diversity')} unique SSH libraries detected.\")\n",
        "    print(f\"Operational Observation: The high frequency of '{short_fp}' suggests a standardized attack toolkit.\")\n",
        "    print(f\"Assessment: Attribution indicates {intel.get('dependency_level')} dependency on a specific automation library.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'df' exists from Phase I\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase VII: Technical Attribution (HASSH)...\")\n",
        "\n",
        "        # 1. Extract raw HASSH telemetry\n",
        "        hassh_telemetry = extract_hassh_telemetry(df)\n",
        "\n",
        "        if hassh_telemetry is not None:\n",
        "            # 2. Aggregate Top Profiles\n",
        "            top_hassh_df = prepare_top_fingerprints(hassh_telemetry)\n",
        "\n",
        "            if not top_hassh_df.empty:\n",
        "                # 3. Visualize\n",
        "                plot_hassh_fingerprints(top_hassh_df)\n",
        "\n",
        "                # 4. Assess & Summarize\n",
        "                hassh_intel = evaluate_hassh_intel(hassh_telemetry, top_hassh_df)\n",
        "                print_hassh_summary(hassh_intel)\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'df' is not defined. Ensure Phase I executed successfully.\")"
      ],
      "metadata": {
        "id": "78ZzJTMTh56M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE VIII: THREAT ACTOR DOSSIER\n",
        "# ==========================================\n",
        "# Consolidating behavioral, technical, and geographic attribution at the Source IP level\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "# Initialize logger for Phase VIII\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_primary_value(series: pd.Series, fallback: str = \"Unknown\") -> str:\n",
        "    \"\"\"Helper function to safely extract the most frequent value (mode) from a Pandas Series.\"\"\"\n",
        "    if series.empty:\n",
        "        return fallback\n",
        "    counts = series.value_counts()\n",
        "    return counts.index[0] if not counts.empty else fallback\n",
        "\n",
        "def build_base_dossier(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Aggregates raw telemetry to establish the baseline Threat Actor Dossier.\n",
        "\n",
        "    Args:\n",
        "        df: Master telemetry dataframe.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Aggregated dossier mapped by 'src_ip', or None if invalid.\n",
        "    \"\"\"\n",
        "    if df.empty or 'src_ip' not in df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Master dataframe missing or lacks 'src_ip'. Cannot build dossier.\")\n",
        "        return None\n",
        "\n",
        "    dossier = df.groupby('src_ip').agg({\n",
        "        'country': 'first',\n",
        "        'session': 'nunique',\n",
        "        'eventid': 'count'\n",
        "    }).rename(columns={'session': 'total_sessions', 'eventid': 'total_events'})\n",
        "\n",
        "    return dossier\n",
        "\n",
        "def integrate_behavior_labels(dossier: pd.DataFrame, cmd_df: pd.DataFrame, features_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Integrates behavioral ML classifications into the dossier.\"\"\"\n",
        "    if features_df.empty or 'predicted_label' not in features_df.columns:\n",
        "        dossier['primary_classification'] = \"N/A (Skipped)\"\n",
        "        return dossier\n",
        "\n",
        "    # Identify the most frequent behavioral classification per Source IP\n",
        "    temp_merge = cmd_df.merge(\n",
        "        features_df[['session_id', 'predicted_label']],\n",
        "        left_on='session',\n",
        "        right_on='session_id'\n",
        "    )\n",
        "\n",
        "    top_class_series = temp_merge.groupby('src_ip')['predicted_label'].agg(\n",
        "        lambda x: get_primary_value(x, fallback=\"Unknown\")\n",
        "    )\n",
        "\n",
        "    # Map back to dossier\n",
        "    dossier['primary_classification'] = top_class_series\n",
        "    dossier['primary_classification'] = dossier['primary_classification'].fillna(\"Unknown\")\n",
        "\n",
        "    return dossier\n",
        "\n",
        "def integrate_technical_fingerprints(dossier: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Integrates HASSH SSH client fingerprints into the dossier.\"\"\"\n",
        "    kex_events = df[df['eventid'] == 'cowrie.client.kex']\n",
        "\n",
        "    if kex_events.empty:\n",
        "        dossier['primary_hassh'] = \"N/A (No KEX Data)\"\n",
        "        return dossier\n",
        "\n",
        "    top_hassh_series = kex_events.groupby('src_ip')['hassh'].agg(\n",
        "        lambda x: get_primary_value(x, fallback=\"Unknown\")\n",
        "    )\n",
        "\n",
        "    dossier['primary_hassh'] = top_hassh_series\n",
        "    dossier['primary_hassh'] = dossier['primary_hassh'].fillna(\"Unknown\")\n",
        "\n",
        "    return dossier\n",
        "\n",
        "def print_dossier_summary(dossier: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints the top threat actors and the tactical intelligence summary.\"\"\"\n",
        "    if dossier.empty:\n",
        "        return\n",
        "\n",
        "    top_10 = dossier.sort_values(by='total_events', ascending=False).head(10)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"### TOP THREAT ACTOR DOSSIER: OPERATIONAL PRIORITY LIST ###\")\n",
        "    print(\"=\"*80)\n",
        "    # Using to_string() ensures clean formatting outside of Jupyter environments\n",
        "    print(top_10.to_string())\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Extracting dynamic stats for the brief\n",
        "    top_actor = top_10.index[0]\n",
        "    total_actors = len(dossier)\n",
        "    top_country = dossier['country'].value_counts().idxmax()\n",
        "\n",
        "    concentration_status = 'concentrated' if (total_actors < 50) else 'distributed'\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### ATTRIBUTION AUDIT: HIGH-RISK SOURCE CLUSTERS ###\")\n",
        "    print(f\"Unique Threat Entities: {total_actors:,} distinct source IPs identified.\")\n",
        "    print(f\"Primary Vector Origin: {top_country}\")\n",
        "    print(f\"Priority Target: {top_actor} (Highest Engagement Volume)\")\n",
        "    print(f\"Operational Assessment: Attribution confirms a {concentration_status} global threat landscape.\")\n",
        "    print(f\"Observation: High correlation between '{top_country}' and automated reconnaissance.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'df', 'cmd_df', and 'features_df' exist from prior phases\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase VIII: Threat Actor Dossier Aggregation...\")\n",
        "\n",
        "        # 1. Base Aggregation\n",
        "        dossier_df = build_base_dossier(df)\n",
        "\n",
        "        if dossier_df is not None:\n",
        "            # 2. Behavioral Attribution\n",
        "            dossier_df = integrate_behavior_labels(dossier_df, cmd_df, features_df)\n",
        "\n",
        "            # 3. Technical Fingerprinting\n",
        "            dossier_df = integrate_technical_fingerprints(dossier_df, df)\n",
        "\n",
        "            # 4. Display & Summarize\n",
        "            print_dossier_summary(dossier_df)\n",
        "\n",
        "    except NameError as e:\n",
        "        logger.error(f\"CRITICAL ERROR: Missing dataframe dependencies. {e}\")"
      ],
      "metadata": {
        "id": "a9_u2DARiNMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE IX: TTP MAPPING (MITRE ATT&CK Framework)\n",
        "# ==========================================\n",
        "# Mapping Observed Command Inputs to the MITRE ATT&CK Matrix\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Optional\n",
        "\n",
        "# Initialize logger for Phase IX\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "# Mapping dictionary allows for rapid updates as threat actor TTPs evolve\n",
        "MITRE_MAPPING_RULES = {\n",
        "    'authorized_keys': 'T1098.004 (Persistence)',\n",
        "    'uname': 'T1082 (Discovery)',\n",
        "    'ls /proc': 'T1082 (Discovery)',\n",
        "    'chmod +x': 'T1222 (Defense Evasion)',\n",
        "    'wget': 'T1105 (Ingress Tool Transfer)',\n",
        "    'curl': 'T1105 (Ingress Tool Transfer)'\n",
        "}\n",
        "DEFAULT_TTP = 'Unknown/General Recon'\n",
        "PALETTE_MITRE = 'rocket'\n",
        "\n",
        "def map_mitre_technique(command: str, rules: Dict[str, str] = MITRE_MAPPING_RULES) -> str:\n",
        "    \"\"\"\n",
        "    Evaluates a command string against known MITRE ATT&CK keyword signatures.\n",
        "\n",
        "    Args:\n",
        "        command: The raw shell command string.\n",
        "        rules: Dictionary of keyword signatures mapped to MITRE TTPs.\n",
        "\n",
        "    Returns:\n",
        "        str: The mapped MITRE technique, or a default fallback.\n",
        "    \"\"\"\n",
        "    if pd.isna(command):\n",
        "        return DEFAULT_TTP\n",
        "\n",
        "    cmd_lower = str(command).lower()\n",
        "    for keyword, ttp in rules.items():\n",
        "        if keyword in cmd_lower:\n",
        "            return ttp\n",
        "\n",
        "    return DEFAULT_TTP\n",
        "\n",
        "def apply_ttp_mapping(cmd_df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Applies the MITRE mapping logic to the command telemetry dataframe.\"\"\"\n",
        "    if cmd_df.empty or 'input' not in cmd_df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Command dataframe is empty or lacks 'input' column.\")\n",
        "        return None\n",
        "\n",
        "    # Use apply with the helper function to map the TTPs\n",
        "    cmd_df['mitre_ttp'] = cmd_df['input'].apply(map_mitre_technique)\n",
        "    return cmd_df\n",
        "\n",
        "def plot_mitre_distribution(cmd_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a horizontal count plot of the observed MITRE ATT&CK techniques.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Assigning y to hue and setting legend=False resolves Seaborn deprecation warnings\n",
        "    sns.countplot(\n",
        "        data=cmd_df,\n",
        "        y='mitre_ttp',\n",
        "        hue='mitre_ttp',\n",
        "        palette=PALETTE_MITRE,\n",
        "        order=cmd_df['mitre_ttp'].value_counts().index,\n",
        "        legend=False,\n",
        "        edgecolor='black',\n",
        "        alpha=0.9\n",
        "    )\n",
        "\n",
        "    plt.title('Phase IX: Observed MITRE ATT&CK Techniques (Tactical Distribution)', fontsize=14)\n",
        "    plt.xlabel('Detection Frequency')\n",
        "    plt.ylabel('MITRE Technique')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_mitre_summary(cmd_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the tactical mapping.\"\"\"\n",
        "    if cmd_df.empty or 'mitre_ttp' not in cmd_df.columns:\n",
        "        return\n",
        "\n",
        "    ttp_counts = cmd_df['mitre_ttp'].value_counts()\n",
        "    primary_technique = ttp_counts.idxmax()\n",
        "    total_unique_ttps = cmd_df['mitre_ttp'].nunique()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"### TACTICAL INTELLIGENCE: MITRE ATT&CK MAPPING ###\")\n",
        "    print(f\"Primary Technique Detected: {primary_technique}\")\n",
        "    print(f\"Operational Observation: Commands mapped successfully to {total_unique_ttps} distinct TTP categories.\")\n",
        "    print(f\"Assessment: Attacker behavior focuses heavily on the '{primary_technique}' stage.\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'cmd_df' exists from Phase I\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase IX: MITRE ATT&CK TTP Mapping...\")\n",
        "\n",
        "        # 1. Apply Mapping\n",
        "        cmd_df = apply_ttp_mapping(cmd_df)\n",
        "\n",
        "        if cmd_df is not None:\n",
        "            # 2. Visualize\n",
        "            plot_mitre_distribution(cmd_df)\n",
        "\n",
        "            # 3. Operational Summary\n",
        "            print_mitre_summary(cmd_df)\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'cmd_df' is not defined. Ensure Phase I executed successfully.\")"
      ],
      "metadata": {
        "id": "15LBwTafiiBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE X: ATTACK VELOCITY (Temporal Heatmap)\n",
        "# ==========================================\n",
        "# Analyzing the temporal distribution of engagement to identify peak threat windows\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Initialize logger for Phase X\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "DAYS_OF_WEEK = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "HEATMAP_CMAP = 'YlGnBu'\n",
        "\n",
        "def prepare_temporal_data(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Extracts temporal features and pivots the dataframe for heatmap visualization.\n",
        "\n",
        "    Args:\n",
        "        df: Master telemetry dataframe containing 'timestamp'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pivoted matrix mapping day of week against hour of day.\n",
        "    \"\"\"\n",
        "    if df.empty or 'timestamp' not in df.columns:\n",
        "        logger.error(\"CRITICAL ERROR: Master dataframe missing or lacks 'timestamp' column.\")\n",
        "        return None\n",
        "\n",
        "    # Isolate temporal features to prevent unintended mutation of the master dataframe\n",
        "    temp_df = df[['timestamp']].copy()\n",
        "\n",
        "    # Ensure timestamp is safely typed as datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(temp_df['timestamp']):\n",
        "        temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'], errors='coerce')\n",
        "        temp_df = temp_df.dropna(subset=['timestamp'])\n",
        "\n",
        "    if temp_df.empty:\n",
        "        logger.warning(\"VISUALIZATION DEFERRED: No valid datetime data available.\")\n",
        "        return None\n",
        "\n",
        "    temp_df['hour'] = temp_df['timestamp'].dt.hour\n",
        "    temp_df['day_name'] = temp_df['timestamp'].dt.day_name()\n",
        "\n",
        "    # Pivot the data into a frequency matrix\n",
        "    pivot_temp = temp_df.groupby(['day_name', 'hour']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Reindex rows to ensure days are ordered logically from Monday to Sunday\n",
        "    pivot_temp = pivot_temp.reindex([d for d in DAYS_OF_WEEK if d in pivot_temp.index])\n",
        "\n",
        "    # Reindex columns to ensure all 24 hours are present on the X-axis\n",
        "    pivot_temp = pivot_temp.reindex(columns=range(24), fill_value=0)\n",
        "\n",
        "    return pivot_temp\n",
        "\n",
        "def plot_attack_velocity(pivot_temp: pd.DataFrame) -> None:\n",
        "    \"\"\"Generates a temporal heatmap of attack velocity.\"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    sns.heatmap(\n",
        "        pivot_temp,\n",
        "        cmap=HEATMAP_CMAP,\n",
        "        cbar_kws={'label': 'Event Frequency'}\n",
        "    )\n",
        "\n",
        "    plt.title('Phase X: Attack Velocity (UTC Heatmap)', fontsize=14)\n",
        "    plt.xlabel('Hour of Day (UTC)')\n",
        "    plt.ylabel('Day of Week')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_temporal_intel(pivot_temp: pd.DataFrame, total_events: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the temporal distribution to generate operational intelligence.\n",
        "\n",
        "    Args:\n",
        "        pivot_temp: The pivoted temporal frequency matrix.\n",
        "        total_events: Total event count from the master dataset.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing the derived temporal intelligence metrics.\n",
        "    \"\"\"\n",
        "    if pivot_temp.empty:\n",
        "        return {}\n",
        "\n",
        "    # Safely extract peak timings from the matrix\n",
        "    peak_day = pivot_temp.sum(axis=1).idxmax()\n",
        "    peak_hour = pivot_temp.sum(axis=0).idxmax()\n",
        "\n",
        "    # Heuristic: Is the max single-hour burst significantly larger than the baseline?\n",
        "    is_concentrated = pivot_temp.max().max() > (pivot_temp.mean().mean() * 2)\n",
        "\n",
        "    return {\n",
        "        \"peak_day\": peak_day,\n",
        "        \"peak_hour\": peak_hour,\n",
        "        \"total_events\": total_events,\n",
        "        \"is_concentrated\": is_concentrated\n",
        "    }\n",
        "\n",
        "def print_temporal_summary(intel: Dict[str, Any]) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the attack velocity.\"\"\"\n",
        "    if not intel:\n",
        "        return\n",
        "\n",
        "    cadence_status = 'concentrated' if intel.get('is_concentrated') else 'distributed'\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"### TEMPORAL INTELLIGENCE: ATTACK VELOCITY AUDIT ###\")\n",
        "    print(f\"Peak Activity Window: {intel.get('peak_day')}s at {intel.get('peak_hour'):02d}:00 UTC\")\n",
        "    print(f\"Total Temporal Events: {intel.get('total_events'):,} engagement points analyzed.\")\n",
        "    print(f\"Operational Observation: Data indicates a {cadence_status} attack cadence.\")\n",
        "    print(\"Assessment: Velocity patterns are consistent with global automated task scheduling (Cron/Scripts).\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'df' exists from Phase I\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase X: Attack Velocity Analysis...\")\n",
        "\n",
        "        # 1. Prepare Data\n",
        "        pivot_df = prepare_temporal_data(df)\n",
        "\n",
        "        if pivot_df is not None:\n",
        "            # 2. Visualize\n",
        "            plot_attack_velocity(pivot_df)\n",
        "\n",
        "            # 3. Assess & Summarize\n",
        "            total_events_count = len(df)\n",
        "            temporal_intel = evaluate_temporal_intel(pivot_df, total_events_count)\n",
        "            print_temporal_summary(temporal_intel)\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'df' is not defined. Ensure Phase I executed successfully.\")"
      ],
      "metadata": {
        "id": "O6uOUJAWi3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE XI: FORENSIC TIMELINE (AI Agent Evidence)\n",
        "# ==========================================\n",
        "# Identification of high-entropy interactive sessions for behavioral auditing\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Initialize logger for Phase XI\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "AI_AGENT_LABEL = \"AI/LLM Agent (Burst-Think)\"\n",
        "MAX_CMD_DISPLAY_LEN = 15\n",
        "\n",
        "def extract_forensic_session(features_df: pd.DataFrame, cmd_df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Isolates the highest-confidence AI agent session for forensic auditing.\n",
        "\n",
        "    Args:\n",
        "        features_df: Dataframe containing session behavioral labels.\n",
        "        cmd_df: Master command telemetry dataframe.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing the isolated session dataframe and the target session ID.\n",
        "    \"\"\"\n",
        "    if features_df.empty or 'predicted_label' not in features_df.columns:\n",
        "        logger.error(\"FORENSIC AUDIT DEFERRED: 'predicted_label' column missing. Phase III likely deferred.\")\n",
        "        return None, None\n",
        "\n",
        "    ai_candidates = features_df[features_df['predicted_label'] == AI_AGENT_LABEL]\n",
        "\n",
        "    if ai_candidates.empty:\n",
        "        # Returns empty DF to signify \"Analysis ran, but no AI found\"\n",
        "        return pd.DataFrame(), None\n",
        "\n",
        "    # Select the session with the highest burst ratio for forensic visualization\n",
        "    target_session = ai_candidates.sort_values('burst_ratio', ascending=False).iloc[0]['session_id']\n",
        "    session_data = cmd_df[cmd_df['session'] == target_session].copy()\n",
        "\n",
        "    if session_data.empty:\n",
        "        return pd.DataFrame(), target_session\n",
        "\n",
        "    # Ensure timestamp is datetime and calculate temporal offset for timeline plotting\n",
        "    session_data['timestamp'] = pd.to_datetime(session_data['timestamp'])\n",
        "    start_time = session_data['timestamp'].min()\n",
        "    session_data['seconds_offset'] = (session_data['timestamp'] - start_time).dt.total_seconds()\n",
        "\n",
        "    return session_data, target_session\n",
        "\n",
        "def plot_forensic_timeline(session_data: pd.DataFrame, target_session: str) -> None:\n",
        "    \"\"\"Generates a 1D temporal scatter plot to visualize command burst intervals.\"\"\"\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # 1D Scatter plot for temporal sequence\n",
        "    sns.scatterplot(\n",
        "        data=session_data,\n",
        "        x='seconds_offset',\n",
        "        y=[1] * len(session_data),\n",
        "        s=150,\n",
        "        color='red',\n",
        "        marker='|'\n",
        "    )\n",
        "\n",
        "    # Add command text annotations safely\n",
        "    for _, row in session_data.iterrows():\n",
        "        cmd_text = str(row.get('input', ''))\n",
        "        display_text = f\"{cmd_text[:MAX_CMD_DISPLAY_LEN]}...\" if len(cmd_text) > MAX_CMD_DISPLAY_LEN else cmd_text\n",
        "        plt.text(row['seconds_offset'], 1.02, display_text, rotation=45, fontsize=8)\n",
        "\n",
        "    plt.title(f'Phase XI: Forensic Timeline - AI Agentic Pattern (Session: {target_session})', fontsize=14)\n",
        "    plt.ylim(0.98, 1.1)\n",
        "    plt.yticks([])  # Hide Y axis as it's a 1-dimensional timeline\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_forensic_summary(session_data: pd.DataFrame, target_session: Optional[str], total_analyzed: int) -> None:\n",
        "    \"\"\"Prints a formatted operational intelligence summary of the forensic audit.\"\"\"\n",
        "    if target_session is None or session_data.empty:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"### FORENSIC AUDIT: NO AI AGENTIC PATTERNS DETECTED ###\")\n",
        "        print(f\"Sample Size: {total_analyzed:,} unique sessions analyzed.\")\n",
        "        print(\"Forensic Observation: All sessions exhibited linear timing or low-complexity scripted behavior.\")\n",
        "        print(\"Assessment: Current harvest consists exclusively of standard automated 'background radiation' botnets.\")\n",
        "        print(\"Conclusion: No evidence of human-agentic or LLM-driven decision making in this data slice.\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        return\n",
        "\n",
        "    max_offset = session_data['seconds_offset'].max()\n",
        "    cmd_count = len(session_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"### FORENSIC AUDIT: HIGH-CONFIDENCE AI PATTERN IDENTIFIED ###\")\n",
        "    print(f\"Session ID: {target_session}\")\n",
        "    print(\"Primary Indicator: Non-Linear Temporal Entropy (Burst-Think)\")\n",
        "    print(f\"Forensic Observation: The actor executed {cmd_count} commands in {max_offset:.2f} seconds.\")\n",
        "    print(\"Assessment: Cadence is inconsistent with human biological processing latency.\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Using to_string() ensures clean formatting across all environments\n",
        "    print(session_data[['seconds_offset', 'time_delta', 'input']].to_string(index=False))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'features_df' and 'cmd_df' exist from prior phases\n",
        "    try:\n",
        "        print(\"\\nInitiating Phase XI: Forensic AI Agent Auditing...\")\n",
        "\n",
        "        # 1. Isolate target session\n",
        "        session_df, target_id = extract_forensic_session(features_df, cmd_df)\n",
        "\n",
        "        if session_df is not None:\n",
        "            total_sessions = len(features_df)\n",
        "\n",
        "            # 2. Visualize & Summarize\n",
        "            if not session_df.empty and target_id:\n",
        "                plot_forensic_timeline(session_df, target_id)\n",
        "\n",
        "            print_forensic_summary(session_df, target_id, total_sessions)\n",
        "\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: Dataframe dependencies missing. Ensure prior phases executed successfully.\")"
      ],
      "metadata": {
        "id": "axsWeKptjJ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE XII: HARVEST SUMMARY REPORT\n",
        "# ==========================================\n",
        "# Final executive readout consolidating the most significant actor profile\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "# Initialize logger for Phase XII\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def generate_harvest_report(dossier: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Generates a high-level executive summary of the most significant threat actor detected.\n",
        "\n",
        "    Args:\n",
        "        dossier: Aggregated threat actor dossier from Phase VIII.\n",
        "    \"\"\"\n",
        "    if dossier.empty:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"### PROJECT HARVEST REPORT: NO DATA FOUND ###\")\n",
        "        print(\"Action Required: Verify data ingestion and Phase VIII dossier construction.\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        return\n",
        "\n",
        "    # Identify the Primary Threat Actor (by event volume)\n",
        "    top_actor_ip = dossier.sort_values(by='total_events', ascending=False).index[0]\n",
        "    actor_stats = dossier.loc[top_actor_ip]\n",
        "\n",
        "    # Extraction with defensive defaults to prevent KeyErrors\n",
        "    origin = actor_stats.get('country', 'Unknown')\n",
        "    classification = actor_stats.get('primary_classification', 'Unclassified')\n",
        "    hassh = actor_stats.get('primary_hassh', 'Unknown')\n",
        "    total_events = actor_stats.get('total_events', 0)\n",
        "    total_sessions = actor_stats.get('total_sessions', 0)\n",
        "\n",
        "    # Print Executive Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### PROJECT HARVEST REPORT: EXECUTIVE SUMMARY ###\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Primary Threat Actor:   {top_actor_ip}\")\n",
        "    print(f\"Origin Node:            {origin}\")\n",
        "    print(f\"Behavioral Profile:     {classification}\")\n",
        "    print(f\"Technical Signature:    {hassh}\")\n",
        "    print(f\"Operational Volume:     {total_events:,} events across {total_sessions:,} sessions\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Final Strategic Assessment - Branching logic based on ML classification\n",
        "    print(\"### STRATEGIC RECOMMENDATION ###\")\n",
        "    if \"Human\" in str(classification):\n",
        "        print(\"ALERT: Interactive behavior detected. Immediate review of session commands is advised.\")\n",
        "        print(\"ACTION: Escalate to Level 3 Incident Response for manual forensic deep-dive.\")\n",
        "    elif \"AI/LLM\" in str(classification):\n",
        "        print(\"ALERT: Advanced agentic behavior identified. Pattern suggests LLM-driven reconnaissance.\")\n",
        "        print(\"ACTION: Update firewall egress rules and rotate credentials targeted in Phase V-D.\")\n",
        "    else:\n",
        "        print(\"STATUS: Background noise/automated probes. No immediate manual action required.\")\n",
        "        print(\"ACTION: Monitor for volume spikes and feed indicators (HASSH/IP) into blocklists.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 'dossier_df' exists from Phase VIII\n",
        "    try:\n",
        "        print(\"Generating Final Phase XII Harvest Report...\")\n",
        "        generate_harvest_report(dossier_df)\n",
        "    except NameError:\n",
        "        logger.error(\"CRITICAL ERROR: 'dossier_df' is not defined. Ensure Phase VIII executed successfully.\")"
      ],
      "metadata": {
        "id": "pjM1RRWVjXdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE XIII: STRUCTURED INTELLIGENCE ASSESSMENT\n",
        "# ==========================================\n",
        "# Quantifying operational impact and triage efficiency gains\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "# Initialize logger for Phase XIII\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GLOBAL CONSTANTS ---\n",
        "LATERAL_INDICATORS = ['ssh', 'scp', 'ftp', 'telnet', 'nc', 'curl', 'wget']\n",
        "CONFIDENCE_THRESHOLD = 50\n",
        "\n",
        "def calculate_operational_efficiency(df: pd.DataFrame, features_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculates metrics related to data reduction and triage efficiency.\n",
        "    \"\"\"\n",
        "    total_sessions = df['session'].nunique() if not df.empty else 0\n",
        "    anomalous_sessions = 0\n",
        "    triage_reduction = 0.0\n",
        "    composition = pd.Series({\"Classification Deferred\": 100.0})\n",
        "\n",
        "    if 'anomaly_score' in features_df.columns:\n",
        "        anomalous_sessions = len(features_df[features_df['anomaly_score'] == -1])\n",
        "        triage_reduction = ((total_sessions - anomalous_sessions) / total_sessions) * 100 if total_sessions > 0 else 0\n",
        "\n",
        "    if 'predicted_label' in features_df.columns:\n",
        "        composition = features_df['predicted_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "    return {\n",
        "        \"total_sessions\": total_sessions,\n",
        "        \"anomalous_sessions\": anomalous_sessions,\n",
        "        \"triage_reduction\": triage_reduction,\n",
        "        \"composition\": composition\n",
        "    }\n",
        "\n",
        "def perform_risk_assessment(cmd_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scans for lateral movement indicators and returns risk-level metrics.\n",
        "    \"\"\"\n",
        "    if cmd_df.empty or 'input' not in cmd_df.columns:\n",
        "        return {\"attempts\": 0, \"indicators\": []}\n",
        "\n",
        "    # Case-insensitive search for egress toolsets\n",
        "    pattern = '|'.join(LATERAL_INDICATORS)\n",
        "    lateral_attempts = cmd_df[cmd_df['input'].str.contains(pattern, na=False, case=False)]\n",
        "\n",
        "    unique_tools = []\n",
        "    if not lateral_attempts.empty:\n",
        "        # Extract the first word of the command as the 'tool'\n",
        "        unique_tools = lateral_attempts['input'].str.split().str[0].str.lower().unique().tolist()\n",
        "\n",
        "    return {\n",
        "        \"attempts\": len(lateral_attempts),\n",
        "        \"indicators\": unique_tools\n",
        "    }\n",
        "\n",
        "def print_final_intelligence_brief(efficiency: Dict[str, Any], risk: Dict[str, Any], sample_size: int) -> None:\n",
        "    \"\"\"Prints a professional, structured final intelligence assessment.\"\"\"\n",
        "\n",
        "    # Confidence Rating Logic based on sample size\n",
        "    confidence = \"HIGH\" if sample_size > CONFIDENCE_THRESHOLD else \"MODERATE\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"PROJECT STATUS: SSH BEHAVIORAL STUDY (LOST PIGLET 1)\")\n",
        "    print(f\"CONFIDENCE RATING: {confidence}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n### TACTICAL TELEMETRY: BEHAVIORAL COMPOSITION ###\")\n",
        "    for label, pct in efficiency['composition'].items():\n",
        "        print(f\"  * {pct:04.1f}% | {label}\")\n",
        "\n",
        "    print(\"\\n### OPERATIONAL IMPACT & ROI: TRIAGE EFFICIENCY ###\")\n",
        "    print(f\"  * Automated Triage: Filtered {efficiency['total_sessions']:,} sessions down to {efficiency['anomalous_sessions']:,} high-value anomalies.\")\n",
        "    print(f\"  * Analyst Workload Reduction: {efficiency['triage_reduction']:.1f}% efficiency gain.\")\n",
        "\n",
        "    print(\"\\n### RISK ASSESSMENT: EGRESS & LATERAL MOVEMENT ###\")\n",
        "    if risk['attempts'] == 0:\n",
        "        print(\"  * STATUS: [SECURE] - No unauthorized egress or tool-transfer attempts detected.\")\n",
        "        print(\"  * Assessment: Activity remains confined to initial reconnaissance/sandboxing.\")\n",
        "    else:\n",
        "        print(f\"  * STATUS: [CRITICAL] - {risk['attempts']} egress attempts identified.\")\n",
        "        primary_tools = \", \".join(risk['indicators'][:3])\n",
        "        print(f\"  * Primary Indicators: {primary_tools}...\")\n",
        "        print(\"  * Assessment: Evidence suggests active attempts at payload ingress or lateral pivoting.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STATUS: REPORT COMPLETE\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Initiating Phase XIII: Structured Intelligence Assessment...\")\n",
        "\n",
        "        # 1. Efficiency Analytics\n",
        "        eff_metrics = calculate_operational_efficiency(df, features_df)\n",
        "\n",
        "        # 2. Risk Analytics\n",
        "        risk_metrics = perform_risk_assessment(cmd_df)\n",
        "\n",
        "        # 3. Output Intelligence Brief\n",
        "        print_final_intelligence_brief(eff_metrics, risk_metrics, len(features_df))\n",
        "\n",
        "    except NameError as e:\n",
        "        logger.error(f\"CRITICAL ERROR: Assessment aborted due to missing dataframes. {e}\")"
      ],
      "metadata": {
        "id": "Ucc7UT-Ajjkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE XIV: STRATEGIC CAMPAIGN CORRELATION\n",
        "# ==========================================\n",
        "# Identifying infrastructure reuse by correlating SSH fingerprints (HASSH)\n",
        "# with tactical command sequences (The Playbook)\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Initialize logger for Phase XIV\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def build_campaign_master(df: pd.DataFrame, cmd_df: pd.DataFrame, features_df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Correlates client fingerprints with command sequences to identify shared infrastructure.\n",
        "    \"\"\"\n",
        "    if df.empty or 'hassh' not in df.columns:\n",
        "        logger.error(\"CAMPAIGN AUDIT DEFERRED: Missing HASSH telemetry.\")\n",
        "        return None\n",
        "\n",
        "    # 1. Build Client Fingerprint Map (HASSH)\n",
        "    client_kex = df[df['eventid'] == 'cowrie.client.kex'][['session', 'hassh', 'src_ip']].drop_duplicates()\n",
        "\n",
        "    # 2. Build Command Sequence Fingerprint (The Playbook)\n",
        "    if not cmd_df.empty:\n",
        "        # Group sessions by exact chronological sequence of commands\n",
        "        seq_map = cmd_df.groupby('session')['input'].apply(\n",
        "            lambda x: \" > \".join(x.astype(str).str.strip())\n",
        "        ).reset_index()\n",
        "        seq_map.columns = ['session', 'cmd_sequence']\n",
        "    else:\n",
        "        seq_map = pd.DataFrame(columns=['session', 'cmd_sequence'])\n",
        "\n",
        "    # 3. Unified Campaign Join\n",
        "    # Standardize to string to ensure reliable merging across types\n",
        "    for frame in [client_kex, seq_map]:\n",
        "        frame['session'] = frame['session'].astype(str)\n",
        "\n",
        "    campaign_master = client_kex.merge(seq_map, on='session', how='inner')\n",
        "\n",
        "    # Join with ML labels from Phase III if available\n",
        "    if not features_df.empty:\n",
        "        f_df = features_df.copy()\n",
        "        f_df['session_id'] = f_df['session_id'].astype(str)\n",
        "        campaign_master = campaign_master.merge(\n",
        "            f_df[['session_id', 'predicted_label']],\n",
        "            left_on='session', right_on='session_id', how='left'\n",
        "        )\n",
        "\n",
        "    return campaign_master\n",
        "\n",
        "def analyze_campaign_clusters(campaign_master: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates data into clusters where multiple IPs use the same Fingerprint + Playbook.\n",
        "    \"\"\"\n",
        "    if campaign_master.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # A \"Campaign\" is identified by a unique HASSH + Command Sequence pair\n",
        "    clusters = campaign_master.groupby(['hassh', 'cmd_sequence']).agg({\n",
        "        'src_ip': 'nunique',\n",
        "        'session': 'count'\n",
        "    }).reset_index().rename(columns={'src_ip': 'unique_ips', 'session': 'total_hits'})\n",
        "\n",
        "    # Filter for Multi-IP Infrastructural Reuse (Botnet Swarms)\n",
        "    return clusters[clusters['unique_ips'] > 1].sort_values('unique_ips', ascending=False)\n",
        "\n",
        "def print_campaign_summary(top_campaigns: pd.DataFrame) -> None:\n",
        "    \"\"\"Prints a formatted strategic assessment of botnet infrastructure reuse.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"### STRATEGIC CAMPAIGN ATTRIBUTION: INFRASTRUCTURE REUSE ###\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not top_campaigns.empty:\n",
        "        # Display the top clusters\n",
        "        print(top_campaigns.head(5).to_string(index=False))\n",
        "\n",
        "        total_campaigns = len(top_campaigns)\n",
        "        max_reach = top_campaigns['unique_ips'].max()\n",
        "        primary_hassh = top_campaigns.iloc[0]['hassh']\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\"### STRATEGIC FINDING: COORDINATED BOTNET ACTIVITY ###\")\n",
        "        print(f\"Campaigns Detected: {total_campaigns} distinct infrastructure clusters.\")\n",
        "        print(f\"Max Campaign Reach: {max_reach} unique IPs sharing a single playbook.\")\n",
        "        print(f\"Infrastructural Concentration: High (Evidence of centralized C2).\")\n",
        "        print(f\"Assessment: The '{primary_hassh[:10]}...' cluster is a high-priority tracking target.\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"### STRATEGIC FINDING: NO CROSS-IP REUSE DETECTED ###\")\n",
        "        print(\"Observation: All sessions used unique sequences or distinct fingerprints.\")\n",
        "        print(\"Assessment: Threat landscape appears primarily opportunistic.\")\n",
        "        print(\"Conclusion: No evidence of large-scale 'Swarm' campaigns identified.\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# EXECUTION BLOCK\n",
        "# ------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Initiating Phase XIV: Strategic Campaign Correlation...\")\n",
        "\n",
        "        # 1. Correlate Infrastructure\n",
        "        master_campaign_df = build_campaign_master(df, cmd_df, features_df)\n",
        "\n",
        "        # 2. Cluster & Analyze\n",
        "        if master_campaign_df is not None:\n",
        "            top_campaign_clusters = analyze_campaign_clusters(master_campaign_df)\n",
        "\n",
        "            # 3. Strategic Report\n",
        "            print_campaign_summary(top_campaign_clusters)\n",
        "\n",
        "    except NameError as e:\n",
        "        logger.error(f\"CRITICAL ERROR: Campaign correlation aborted. Dependency error: {e}\")"
      ],
      "metadata": {
        "id": "I394Iv0tjwM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}